# -*- coding: utf-8 -*-
"""Mid-Semester Project: Sports Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13wwQRGm56bxHJbcp7AfEkuzu4DtJSaDx

# importing all necessary libraries
"""

import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn import tree, metrics
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn import svm
from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings(action="ignore", message="^internal gelsd")
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')
from google.colab import drive
drive.mount('/content/drive')

"""# Loading the data, that is the players_21.csv file containing the features for predicting player ratings"""

players_ratings21=pd.read_csv('/content/drive/My Drive/Colab Notebooks/fifa_r/players_21.csv')

players_ratings21.head()

players_ratings21.shape

"""# Data Pre-processing"""

#Dropping all irrelevant columns before cleaning the data

players_ratings21 = players_ratings21.drop(
    ['long_name', 'player_url','club_jersey_number','nation_jersey_number',
     'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url','short_name','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk',
     'rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ls','st','rs','lw','lf','cf','rf'],
    axis=1
)
players_ratings21.head()

#Displaying the number of entities in each column, finding the mean, std, min and the max values as well as displaying the lower and upper quartiles
players_ratings21.describe()

#player stat features
columns = ['sofifa_id', 'overall','movement_reactions','passing','mentality_composure','dribbling',
'potential','release_clause_eur','wage_eur','value_eur',
'power_shot_power','physic','mentality_vision','attacking_short_passing','shooting','goalkeeping_speed',
'skill_long_passing','age','skill_ball_control','international_reputation','league_level', 'nation_team_id','sofifa_id','overall',
            'attacking_crossing','dob','club_joined',
           'attacking_finishing', 'club_name','club_position',
           'attacking_heading_accuracy', 'league_name','club_loaned_from',
           'attacking_short_passing', 'attacking_volleys','club_contract_valid_until',
           'skill_dribbling', 'skill_curve', 'skill_fk_accuracy','league_level', 'nation_team_id',
           'skill_long_passing', 'skill_ball_control', 'movement_acceleration',
           'movement_sprint_speed', 'movement_agility', 'movement_reactions',
           'movement_balance', 'power_shot_power', 'power_jumping', 'power_stamina',
           'power_strength', 'power_long_shots', 'mentality_aggression', 'mentality_interceptions',
           'mentality_positioning', 'mentality_vision', 'mentality_penalties', 'mentality_composure'
           , 'defending_standing_tackle', 'defending_sliding_tackle', 'goalkeeping_diving',
           'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning', 'goalkeeping_reflexes','player_positions']
X = players_ratings21[columns]
X.head()

#Checking if there are missing values before imputation
players_ratings21.isna().any()

# Using the Simple Imputer to fill all missing values before encoding
imputer = SimpleImputer(strategy='most_frequent')

imputed_data = imputer.fit_transform(X)

imputed_df = pd.DataFrame(imputed_data, columns=X.columns)
imputed_df

#Checking if there are still missing values, ie checking if the imputation was successful before encoding
imputed_df.isna().any()

#One-hot encoding using the get_dummies method
X = pd.get_dummies(imputed_df, columns=['overall',
            'attacking_crossing','dob','club_joined',
           'attacking_finishing', 'club_name','club_position',
           'attacking_heading_accuracy', 'league_name','club_loaned_from',
           'attacking_short_passing', 'attacking_volleys','club_contract_valid_until',
           'skill_dribbling', 'skill_curve', 'skill_fk_accuracy',
           'skill_long_passing', 'skill_ball_control', 'movement_acceleration',
           'movement_sprint_speed', 'movement_agility', 'movement_reactions',
           'movement_balance', 'power_shot_power', 'power_jumping', 'power_stamina',
           'power_strength', 'power_long_shots', 'mentality_aggression', 'mentality_interceptions',
           'mentality_positioning', 'mentality_vision', 'mentality_penalties', 'mentality_composure'
           , 'defending_standing_tackle', 'defending_sliding_tackle', 'goalkeeping_diving',
           'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning', 'goalkeeping_reflexes','player_positions'])

#Displaying the encoded dataset
X.head()

"""# A correlation analysis to find the features which show better correlation with the overall rating"""

#The correlation analysis
corr_matrix = players_ratings21.corr()
corr_matrix['overall'].sort_values(ascending=False)

"""# Using the best features according to the correlation analysis"""

#player stat features
columns = ['overall','potential','movement_reactions','passing','mentality_composure','dribbling',
'release_clause_eur','wage_eur','value_eur',
'power_shot_power','physic','mentality_vision','attacking_short_passing','shooting','goalkeeping_speed',
'skill_long_passing','age','skill_ball_control','international_reputation','league_level', 'nation_team_id','sofifa_id']
players_ratings21 = players_ratings21[columns]
players_ratings21.head()

#Getting the dataset information
players_ratings21.info()

#Displaying the number of entities in each column, finding the mean, std, min and the max values as well as displaying the lower and upper quartiles
players_ratings21.describe()

# Using the Simple Imputer to fill all missing values
imputer = SimpleImputer(strategy='most_frequent')

imputed_data = imputer.fit_transform(players_ratings21)

imputed_df = pd.DataFrame(imputed_data, columns=players_ratings21.columns)
imputed_df

#Checking if there are some missing values
imputed_df.isna().any()

# Commented out IPython magic to ensure Python compatibility.
#Plotting histograms to show how the dependent variables are related to the independent variable
# %matplotlib inline
import matplotlib.pyplot as plt
fifa_r.hist(bins=50, figsize=(20,15))
plt.show()

#Plotting scatterplots to show how the dependent variables are related to the independent variable
from pandas.plotting import scatter_matrix
attributes = ['movement_reactions','passing','mentality_composure','dribbling',
'potential','release_clause_eur','wage_eur','value_eur',
'power_shot_power']
scatter_matrix(imputed_df[attributes], figsize=(15,12))
plt.show()

"""# Dividing the dataset into X, the dependent variable and Y the independent variable"""

Y = imputed_df['overall']
X = imputed_df.drop('overall', axis=1)

X = pd.get_dummies(imputed_df)
X

#Scaling the X, the dependent variable using the standard scaler
X=StandardScaler().fit_transform(X)
X

#Displaying the number of Y to check if there is no error
print(Y.value_counts())

"""# Splitting the dataset into training and Testing sets"""

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)

#Using a decision tree classifier
decision_tree=DecisionTreeClassifier(criterion='entropy',random_state=42)

#Applying soft voting
voting_classifier = VotingClassifier(estimators=[
    ('decision_tree', decision_tree),
], voting='soft')

for model in (decision_tree,voting_classifier):
  model.fit(Xtrain,Ytrain)
  y_pred=model.predict(Xtest)
  print(model.__class__.__name__,accuracy_score(y_pred,Ytest))

#training with cross-validation using RandomForest Regressor
from sklearn.ensemble import RandomForestRegressor
forest_reg = RandomForestRegressor()
forest_reg.fit(Xtrain, Ytrain)

#Checking the mean_squared error of the model
y_predictions = forest_reg.predict(Xtrain)
forest_mse = mean_squared_error(Ytrain, y_predictions)
forest_rmse = np.sqrt(forest_mse)
forest_rmse

#Finding the coefficient of determination of the model
from sklearn.metrics import r2_score

# Make predictions on the training set
y_pred_train = forest_reg.predict(Xtrain)

# Calculate the R-squared score
r2 = r2_score(Ytrain, y_pred_train)
print("RandomForest R-squared score:", r2)

from sklearn.model_selection import GridSearchCV
param_grid = [
{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
{'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]},
]
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
scoring='neg_mean_squared_error',
return_train_score=True)
grid_search.fit(Xtrain, Ytrain)

grid_search.best_estimator_

final_model = grid_search.best_estimator_
final_predictions = final_model.predict(Xtest)
final_mse = mean_squared_error(Ytest, final_predictions)
final_rmse = np.sqrt(final_mse)
final_rmse

from sklearn.metrics import r2_score

# Access the best model from the grid search
best_model = grid_search.best_estimator_

# Make predictions on the training set using the best model
y_pred_train = best_model.predict(Xtrain)

# Calculate the R-squared score on the training set
r2_train = r2_score(Ytrain, y_pred_train)
print("R-squared score on training set:", r2_train)

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Instantiate the Gradient Boosting Regressor
gb_regressor = GradientBoostingRegressor()

# Fit the model to the training data
gb_regressor.fit(Xtrain, Ytrain)

# Generate predictions on the test data
y_pred = gb_regressor.predict(Xtest)

# Evaluate the model
mse = mean_squared_error(Ytest, y_pred)
r2 = r2_score(Ytest, y_pred)

print("Mean Squared Error (MSE):", mse)
print("Coefficient of Determination (R-squared):", r2)

import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from google.colab import drive
drive.mount('/content/drive')

# Load the "players_22" dataset
players_22=pd.read_csv('/content/drive/My Drive/Colab Notebooks/fifa_r/players_22.csv')

players_22

# Preprocess the new data (make sure to apply the same preprocessing as done on the training data)

# Extract the features from the "players_22" dataset
#player stat features
columns = ['overall','potential','movement_reactions','passing','mentality_composure','dribbling',
'release_clause_eur','wage_eur','value_eur',
'power_shot_power','physic','mentality_vision','attacking_short_passing','shooting','goalkeeping_speed',
'skill_long_passing','age','skill_ball_control','international_reputation','league_level', 'nation_team_id','sofifa_id']
players_22 = players_22[columns]
players_22.head()

imputer = SimpleImputer(strategy='most_frequent')

imputed_d = imputer.fit_transform(players_22)

imputed_d = pd.DataFrame(imputed_data, columns=players_22.columns)
imputed_d

X_new = imputed_d.drop('overall', axis=1)
X_new

X_new = pd.get_dummies(imputed_d)
X_new

X_new=StandardScaler().fit_transform(X_new)
X_new

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Create a new instance of RandomForestRegressor
forest_reg = RandomForestRegressor()

# Fit the model with the training data
forest_reg.fit(Xtrain, Ytrain)

# Generate predictions on the new data
players_22_predictions = forest_reg.predict(X_new)

# Compare the predicted values with the actual target values
mse = mean_squared_error(Y, players_22_predictions)
r2 = r2_score(Y, players_22_predictions)

print("Mean Squared Error (MSE) on new data:", mse)
print("Coefficient of Determination (R-squared) on new data:", r2)

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Instantiate the Gradient Boosting Regressor
gb_regressor = GradientBoostingRegressor()

# Fit the model to the training data
gb_regressor.fit(Xtrain, Ytrain)

# Generate predictions on the new data
players_22_predictions = forest_reg.predict(X_new)

# Compare the predicted values with the actual target values
mse = mean_squared_error(Y, players_22_predictions)
r2 = r2_score(Y, players_22_predictions)

print("Mean Squared Error (MSE):", mse)
print("Coefficient of Determination (R-squared):", r2)

import pickle

# Train the model
model = RandomForestRegressor()
model.fit(Xtrain, Ytrain)

# Save the trained model to a file
with open('trained_model.pkl', 'wb') as f:
    pickle.dump(model, f)

import pickle

# Train the model
model = GradientBoostingRegressor()
model.fit(Xtrain, Ytrain)

# Save the trained model to a file
with open('trained_model.pkl', 'wb') as f:
    pickle.dump(model, f)

import streamlit as st
import joblib

# Load the trained model
model = joblib.load('trained_model.pkl')

# Define a function for model prediction
def predict(data):
    # Preprocess the input data (assuming preprocessing steps are defined)
    preprocessed_data = preprocess(data)

    # Make predictions using the preprocessed data
    predictions = model.predict(preprocessed_data)

    return predictions

# Create a Streamlit app
def main():
    # Add a title and description
    st.title('Model Prediction')
    st.write('Enter the input data for prediction:')

    # Create input fields for the data
    input_data = {}
    input_data['feature1'] = st.number_input('Feature 1')
    input_data['feature2'] = st.number_input('Feature 2')
    # Add more input fields as needed

    # Perform prediction when a button is clicked
    if st.button('Predict'):
        # Call the predict function with the input data
        result = predict([input_data])

        # Display the prediction result
        st.write('Prediction:', result)

if __name__ == '__main__':
    # Run the Streamlit app
    main()